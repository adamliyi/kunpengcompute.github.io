{"pages":[{"title":"关于鲲鹏","text":"什么是鲲鹏计算产业？ 鲲鹏计算产业是基于Kunpeng处理器构建的全栈IT基础设施、行业应用及服务，包括PC、服务器、存储、操作系统、中间件、虚拟化、数据库、云服务、行业应用以及咨询管理服务等。 华为作为鲲鹏计算产业的成员，聚焦于发展华为鲲鹏+昇腾双引擎芯片族，通过“硬件开放、软件开源、使能合作伙伴”来推动计算产业的发展。硬件厂商基于开放的服务器主板和PC主板发展自有品牌的产品和解决方案，软件厂商基于 openEuler的开源OS以及配套的数据库、中间件等平台软件发展应用软件和服务。 鲲鹏计算产业目标是建立完善的开发者和产业人才体系，通过产业联盟、开源社区、OpenLab、行业标准组织一起完善产业链，打通行业全栈，使鲲鹏生态成为开发者和用户的首选。","link":"/about/index.html"}],"posts":[{"title":"ARM CPU Vendor 及 Part ID 映射关系（持续更新）","text":"作者：郑振宇 根据ARM CPU官方技术手册，ARM CPU的CPU型号、Vendor、版本等信息存于MIDR_EL1寄存器中:其中从低至高第0-3 bit表示revision，代表固件版本的小版本号，如r1p3中的p3；第4-15 bit表示part number(id)，代表这款CPU在所在vendor产品中定义的产品代码，如在HiSilicon产品中，part_id=0xd01代表Kunpeng-920芯片；第16-19 bit表示architecture，即架构版本，0x8即ARMv8；第20-23 bit表示variant，即固件版本的大版本号，如r1p3中的r1；第24-31 bit表示implementer，即vendor id，如vendor_id=0x48表示HiSilicon。 想要知道一款ARM CPU的具体型号，则需要首先解析vendor_id(implementer) 然后再在该Vendor的所有型号中匹配part_id，才能获取到具体的信息；这里列出目前系统中已有的Vendor列表和其ID对应关系，以及主流厂商的主要型号映射关系： Vendor映射关系： Vendor Name Vendor ID ARM 0x41 Broadcom 0x42 Cavium 0x43 DigitalEquipment 0x44 HiSilicon 0x48 Infineon 0x49 Freescale 0x4D NVIDIA 0x4E APM 0x50 Qualcomm 0x51 Marvell 0x56 Intel 0x69 型号映射关系ARM Part ID Model Name 0xd03 Cortex-a53 0xd07 Cortex-a57 0xd08 Cortex-a72 Broadcom Part ID Model Name 0x0f Brahma B15 0x100 Brahma B53 Cavium Part ID Model Name 0x0af Thunder X2 29xx Qualcomm Part ID Model Name 0xc00 Falkor HiSilicon Part ID Model Name 0xd01 Kunpeng-920","link":"/2020/04/03/arm-cpu-vendor-ji-part-id-ying-she-guan-xi-chi-xu-geng-xin/"},{"title":"ARM优化和Java Math库有关的那些坑","text":"作者：姜逸坤 1. 起初最近在进行ARM切换的过程中发现了很多因为Java Math库在不同的平台上的精度不同导致用例失败，我们以Math.log为例，做一下简单的分析。下面是一个简单的计算log(3)的示例： 123456public class Hello { public static void main(String[] args) { System.out.println(\"Math.log(3): \" + Math.log(3)); System.out.println(\"StrictMath.log(3): \" + StrictMath.log(3)); }} 我们发现，在x86下，Math的结果为1.0986122886681098。 1234# on x86$ java HelloMath.log(3): 1.0986122886681098StrictMath.log(3): 1.0986122886681096 而aarch64的结果为1.0986122886681096。 1234# on aarch64$ java HelloMath.log(3): 1.0986122886681096StrictMath.log(3): 1.0986122886681096 而在Java 8的官方文档中，对此有明确说明： Unlike some of the numeric methods of class StrictMath, all implementations of the equivalent functions of class Math are not defined to return the bit-for-bit same results. This relaxation permits better-performing implementations where strict reproducibility is not required. 因此，结论是：Math的结果有可能是不精确的，如果结果对精度有苛求，那么请使用StrictMath。 在此，我们留下2个疑问： 为什么说Math的实现不是the bit-for-bit same results？ Math是怎么实现在各个架构下better-performing implementations的？ 2. 深度探索一下Math的实现为了能够更清晰的看到StrictMath的实现，我们深入的看了下JDK的实现。 2.1 Math和StrictMath的基本实现我们从Math.log和StrictMath.log的实现为例，进行深入学习： Math.log的代码表面上很简单，就是直接调用StrictMath.log。123public static double log(double a) { return StrictMath.log(a); // default impl. delegates to StrictMath} StrictMath的代码，会调用StrictMath.c中的方法，最终会调用fdlibm的e_log.c的实现。 总体的实现和下图类似： 对于StrictMath来说，没有什么黑科技，最终的实现就是e_log.c的ieee754标准实现，是通过C语言实现的，所以在各个平台的表现是一样的，整个流程如图中蓝色部分。感兴趣的同学可以看e_log.c的源码实现即可。 2.2 Math的黑科技回到我们最初的起点，再加上一个问题： 为什么说Math的实现不是the bit-for-bit same results？ Math是怎么实现在各个架构下better-performing implementations的？ 既然Math的实现，也是直接调用StrictMath，为什么结果确不一样呢？ 原来，JVM为了让各个arch的CPU能够充分的发挥自己CPU的优势，会根据架构不同，会通过Hotspot intrinsics替换掉Math函数的实现，我们可以从代码vmSymbols.hpp看到，Math的很多实现都被替换掉了。log的替换类似于： 1do_intrinsic(_dlog, java_lang_Math, log_name, double_double_signature, F_S) 最终，Math的调用为下图红色部分： log的实现: 在x86下，最终其实调用的是assembler_x86.cpp中的flog实现:12345void Assembler::flog() { fldln2(); fxch(); fyl2x();} 而在aarch64下，我们可以从src/hotspot/cpu/目录下看到，aarch64并未实现优化版本。因此，实际aarch64调用的就是标准的StrictMath。 正因如此，x86汇编的计算结果的差异导致了x86和aarch64结果在Math.log差异。 当然，aarch64也在JDK 11中，对部分的Math接口做了加速实现，有兴趣可以看看JEP 315: Improve Aarch64 Intrinsics的实现。 3. toRadians的小插曲在ARM优化过程中，有的是因为Math库和StrictMath不同的实现造成结果不同，所以我们如果对精度要求非常高，直接切到StrictMath即可。 但有的函数，由于在Java大版本升级的过程中，出现了一些实现的差异，先看一个简单的Java程序 123456public class Hello { public static void main(String[] args) { System.out.println(\"Math.toRadians(0.33): \" + Math.toRadians(0.33)); System.out.println(\"StrictMath.toRadians(0.33): \" + StrictMath.toRadians(0.33)); }} 我们分别看看在Java11和Java8的结果： 123$ /usr/lib/jvm/java-11-openjdk-amd64/bin/java HelloMath.toRadians(0.33): 0.005759586531581287StrictMath.toRadians(0.33): 0.005759586531581287 123$ /usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java HelloMath.toRadians(0.33): 0.005759586531581288StrictMath.toRadians(0.33): 0.005759586531581288 最后一位很奇怪的差了1，我们继续深入进去看到toRadians的实现： Java8的实现为：1234// Java 8 public static double toDegrees(double angrad) { return angrad * 180.0 / PI;} Java11的实现为：1234private static final double DEGREES_TO_RADIANS = 0.017453292519943295;public static double toRadians(double angdeg) { return angdeg * DEGREES_TO_RADIANS;} 原来在Java11的实现中，为了优化性能，将* 180.0 / PI提前算好了，这样每次只用乘以乘数即可，从而化简了计算。这也最终导致了，Java8和Java11在精度上有一些差别。 4. 总结 Math在各个arch下的实现不同，精度也不同，如果对精度要求很高，可以使用StrictMath。 Java不同版本的优化，也有可能导致Math库的精度不同 Math库在实现时，利用intrinsics机制，把各个arch下Math的实现换掉了，从而充分的发挥各个CPU自身的优势。","link":"/2020/04/08/arm-you-hua-he-java-math-ku-you-guan-de-na-xie-keng/"},{"title":"Linux下获取ARMv8-A CPU详情的3种方法","text":"作者：郑振宇 在ARM平台上进行软件适配时，经常遇到需要根据不同CPU的具体型号、额外属性等信息进行分支处理的需求，因而需要获取CPU的详情信息；ARM架构CPU与X86架构芯片在CPU详情信息的呈现上有很大不同。本文将简述ARM CPU与CPU详情相关的知识及在Linux下获取ARMv8-A CPU详情的三种方法。 ARM CPU中有关CPU详情的寄存器根据ARM CPU官方技术手册，ARM CPU的CPU型号、Vendor、版本等信息存于MIDR_EL1寄存器中:其中从低至高第0-3 bit表示revision，代表固件版本的小版本号，如r1p3中的p3；第4-15 bit表示part number(id)，代表这款CPU在所在vendor产品中定义的产品代码，如在HiSilicon产品中，part_id=0xd01代表Kunpeng-920芯片；第16-19 bit表示architecture，即架构版本，0x8即ARMv8；第20-23 bit表示variant，即固件版本的大版本号，如r1p3中的r1；第24-31 bit表示implementer，即vendor id，如vendor_id=0x48表示HiSilicon。 想要知道一款ARM CPU的具体型号，则需要首先解析vendor_id(implementer) 然后再在该Vendor的所有型号中匹配part_id，才能获取到具体的信息；这里列出目前系统中已有的Vendor列表和其ID对应关系 Vendor Name Vendor ID ARM 0x41 Broadcom 0x42 Cavium 0x43 DigitalEquipment 0x44 HiSilicon 0x48 Infineon 0x49 Freescale 0x4D NVIDIA 0x4E APM 0x50 Qualcomm 0x51 Marvell 0x56 Intel 0x69 而对于具体型号来说，对应关系则更为复杂，这里就不一一列举，可以参考本站文章或util-linux/lscpu工具中的相关具体实现来获取完整的映射关系，lscpu工具我们则将在后面的部分中进行介绍。 上面介绍过，除了CPU型号之外，我们通常还会关注CPU是否支持我们需要的特性(扩展指令集，CPU Flags, CPU features)；与X86相差较大(CPU features定义集中在EBX,ECX和EDX寄存器中)，ARM架构的这些特性分散于ID_PFR0_EL1, ID_PFR1_EL1, ID_DFR0_EL1, ID_ISAR0_EL1 等等若干个专用寄存器中，解析起来难度较高，后面我们会详细讨论如何获取这些内容。 在Linux下如何获取CPU详情信息在介绍具体的方法前，首先需要介绍一下ARMv8架构下的安全分层机制(Exception Level):如上图所示，ARMv8架构是专为数据中心场景而设计的架构，相比较早的ARM架构，新增了EL2层用于实现硬件虚拟化；较高层的用户是无权直接限访问下一层的数据内容的，对于我们的场景来说，由上面介绍的内容中可以看到，前面所有介绍的寄存器都存在于EL1层，而我们通常使用的应用程序都处于EL0层，因此是无法直接访问到这些寄存器的。那么该如何读取这些内容呢？ 1. 从文件节点获取OS在启动时，会将底层硬件信息载入到相应的文件节点中，这样，位于EL0层的用户就可以通过读取这些文件节点来获取这些信息，比较常用的有两个： /sys/devices/system/cpu:该文件夹下保存了较全的CPU信息文件，并按单个CPU进行区分，读取其中某一个的regs文件目录就可以获得相应的CPU详情信息，如我们尝试获取CPU0的相关信息：可以看到，我们读取的仍然是MIDR_EL1寄存器相对应的信息，并且是未解析的数据，需要对应上文介绍的方法进行解析。并且目前没有在这个文件夹下找到CPU Flags的相关的信息，如果后续找到其所在位置，会刷新。 /proc/cpuinfo:通过读取cpuinfo可以看到，通过这种方法获取的CPU详情，是进行过解析的，对原始数据进行了拆分，并且是包含了CPU Flag信息的，但仍与X86下的结果有较大不同，各个key所对应的信息仍然需要根据表单进行解析才能转变为人为可读的信息。 2.使用LSCPU命令读取Linux内核的开发者显然也发现了上文介绍的两种方法获取信息不够全面且需要二次解析的问题，因此在Linux外围工具组util-linux/lscpu(wiki)中进行了改进，从2.32版本开始增加了对ARM平台CPU信息的解析，从而提供人为可读的内容(由于2019年11月才合入相关Patch，HiSilicon芯片的解析需要手动编译最新主干代码才能实现)。从上图可以看到，lscpu提供了非常丰富且直观的内容。 3.使用内联汇编和辅助向量直接解析上文介绍的两种方法相对来说比较简单，但需要进行读取文件、运行外部命令等操作；当想在自己的程序中引用上述信息时，速度会相对较慢且会引入新的依赖(lscpu)。因此最快速的方法是通过内联汇编直接读取并解析相应的寄存器；上文中已经提到，用户在EL0无法直接读取到位于EL1的寄存器内的内容，那么该如何去做呢？ ARM已经为我们准备好了一切，用户可以通过MRS指令将程序状态寄存器的内容传送到通用寄存器中，再进行进一步的解析，因此我们可以这样做： 12345678/* read the cpuid data from MIDR_EL1 register */asm(\"mrs %0, MIDR_EL1\" : \"=r\" (cpuid));VIR_DEBUG(\"CPUID read from register: 0x%016lx\", cpuid);/* parse the coresponding part_id bits */data-&gt;pvr = cpuid&gt;&gt;4&amp;0xFFF;/* parse the coresponding vendor_id bits */data-&gt;vendor_id = cpuid&gt;&gt;24&amp;0xFF; 这样就可以快速的获取CPUID相关的具体内容，在根据表格进行映射即可获得Vendor和Model信息； 对于CPU Flags，由于牵扯到的寄存器众多，读者可以根据ARM64 CPU Feature Registers中的示例程序进行依次进行寄存器读取，再根据相应的映射关系进行解析，也可以使用下面将要介绍的可读性更高的另一种方法。 Linux内核提供了getauxval()方法，用于读取辅助向量(auxiliary vector, 一个从内核到用户空间的信息交流机制)，通过读取相应的辅助向量，我们就能获取相应的硬件信息；辅助向量有很多，感兴趣的读者可以查看上面的链接，对于我们读取CPU Flags来说，关心的是AT_HWCAP这个辅助向量，通过getauxval()读取这个向量的值，可以获得整合过的CPU Flags信息，其bit定位规则则在hwcap.h，当然，每种架构下的对应关系不相同，需要根据需要进行查找。 那么，我们就可以采用下面的方法进行解析： 12345678910111213141516171819202122#include &lt;stdio.h&gt;#include &lt;sys/auxv.h&gt;/* 通过移位Bit Mask来读取相应的标志位 */#define BIT_SHIFTS(n) (UL(1) &lt;&lt; (n))int main() { unsigned long hwcaps = getauxval(AT_HWCAP); int i; /* 目前ARMv8架构只有32种CPU Flags */ char *list[32] = {\"fp\\n\", \"asimd\\n\", \"evtstrm\\n\", \"aes\\n\", \"pmull\\n\", \"sha1\", \"sha2\\n\", \"crc32\\n\", \"atomics\\n\", \"fphp\\n\", \"asimdhp\\n\", \"cpuid\\n\", \"asimdrdm\\n\",\"jscvt\\n\", \"fcma\\n\", \"lrcpc\\n\", \"dcpop\\n\", \"sha3\\n\", \"sm3\\n\", \"sm4\\n\", \"asimddp\\n\" , \"sha512\\n\", \"sve\\n\", \"asimdfhm\\n\", \"dit\\n\", \"uscat\\n\", \"ilrcpc\\n\", \"flagm\\n\", \"ssbs\\n\", \"sb\\n\", \"paca\\n\",\"pacg\\n\",}; for (i = 0; i&lt; 32; i++){ if (hwcaps &amp; BIT_SHIFTS(i)) { printf(\"%s\\n\",list[i]); } }} 或者，可以直接通过hwcap.h中预先定义好的宏来做bit mask: 12345678910111213141516171819202122232425#include &lt;stdio.h&gt;#include &lt;sys/auxv.h&gt;#include &lt;asm/hwcap.h&gt;int main(){ long hwcaps= getauxval(AT_HWCAP); if(hwcaps &amp; HWCAP_AES){ printf(\"AES instructions are available\\n\"); } if(hwcaps &amp; HWCAP_CRC32){ printf(\"CRC32 instructions are available\\n\"); } if(hwcaps &amp; HWCAP_PMULL){ printf(\"PMULL/PMULL2 instructions that operate on 64-bit data are available\\n\"); } if(hwcaps &amp; HWCAP_SHA1){ printf(\"SHA1 instructions are available\\n\"); } if(hwcaps &amp; HWCAP_SHA2){ printf(\"SHA2 instructions are available\\n\"); } return 0;}","link":"/2020/04/03/linux-xia-huo-qu-armv8-a-cpu-xiang-qing-de-3-chong-fang-fa/"},{"title":"MySQL on x86 vs ARM","text":"作者: Krunal Bauskar By and large this would be a topic of interest for most of us including me when I started to explore this space. Before we dwell into the numbers let’s first understand some basic differences between 2 architectures. Beyond being CISC and RISC let’s look at the important differences from MySQL perspective. Strong vs Weak memory model (weak memory model needs proper memory barrier while writing lock-free code). Underlying hardware specific specialized instructions. For example: both now support crc32c hardware instructions but being low-level they are different ways to invoke them. For more differences checkout for x86-SSE/ARM-ACLE. Cache Line differences. Most of the ARM processors tend to use bigger cache lines (128 bytes for all caches or a mix of 64/128 bytes). Other sys-call level differences like: absence of PAUSE instructions with ARM and substitute instruction with very low latency failing to induce needed delay, sched_getcpu is costlier on ARM introducing challenges with use of lock-free construct, memory operations seems to show higher latency, etc… Community has contributed multiple patches around this space (Topic for another blog). Since MySQL just started supporting MySQL on ARM there are few optimizations but most of the work is yet to be done. PerformanceNow let’s look at the most important aspect: Performance We tested the performance of MySQL (current release 8.0.19) on x86 and ARM. Details of the test and machine are given below. Test Setup 24 vCPU/48 GB Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz for running MySQL on x86. 24 vCPU/48 GB ARM @ 2.60GHz for running MySQL on ARM sysbench is running on a dedicated machine located in the same data-center. sysbench steps: Load Tables. (Same seed db is reused for multiple runs so warmup is needed). Checksum based warmup. Run checksum on all tables. For checksum, flow needs to fetch the rows in the buffer pool there-by causing it to warm up. Query based warm up. Can skip but helpful if you are using adaptive hash indexes. Execute TC (oltp-read-write/oltp-update-index/oltp-update-non-index/oltp-read-only/oltp-point-select) Each TC is executed for N different scalability. Given 24 vCPU tried it for 1/2/4/8/16/32/128/256. Before switching TC, an intermediate sleep is introduced to help flush changes from previous TC. This can’t ensure all changes are flushed but sleep of X secs ensures least impact on followup TC. MySQL-Server Configuration: BP is large enough to accomodate complete data in-memory For more details please check the following configuration details Details of running the scripts and automated test-script to invoke sysbench are also [available here](https://github.com/mysqlonarm/benchmark-suites) Run specific details: Table: 96-tables * 1.5 million (data-size= 34GB) Buffer Pool: 36GB Redo-Log: 4GB*2 TC-run-time: 300 secs TC-warmup: 60 (sysbench –warmup-time) workload-query-based warmup: 600 change-over-sleep: 180 checksum-based-warmup: enabled data-storage: 300GB (support for 16500 IOPS (nullify effect of Burst IOPS)). Note: Frequency Scaling (FS). Given ARM is running @ 2.6 GHz vs x86 is running @ 3.0 GHz. Comparing them directly is not fair. In order to compensate for the frequency difference, graphs also add frequency-scaled tps/qps for ARM (ARM-fscaled simply extrapolate original ARM tps/qps number by (3/2.6) factor). In real life, the factor could be a bit on the higher side given increasing CPU frequency can affect contention graphs and wait cycles. 1. Point Select: threads ARM (qps) x86 (qps) ARM (qps - fscaled (FS)) % ARM-vs-x86 % ARM (FS)-vs-x86 1 6696 6439 7726 4 20 2 12482 11774 14402 6 22 4 23881 21308 27555 12 29 8 45993 42110 53069 9 26 16 88517 81239 102135 9 26 32 142974 136724 164970 5 21 64 198839 212484 229430 -6 8 128 217778 241555 251282 -10 4 256 209797 224009 242073 -6 8 Analysis: ARM performs better than x86 for lower scalability but fails to scale at same rate with increasing scalability. With frequency scaling applied ARM continues to beat x86 despite of the scalability issues. 2. Read Only: threads ARM (qps) x86 (qps) ARM (qps - fscaled (FS)) % ARM-vs-x86 % ARM (FS)-vs-x86 1 5222 5259 6025 -1 15 2 10333 10200 11923 1 17 4 19176 19349 22126 -1 14 8 36881 37035 42555 0 15 16 70337 67065 81158 5 21 32 109207 113210 126008 -4 11 64 139294 164148 160724 -15 -2 128 151382 175872 174672 -14 -1 256 149136 164382 172080 -9 5 Analysis: ARM is almost on par with x86 for lower scalability but again fails to scale for higher scalability. With frequency scaling applied ARM continues to beat x86 (in most cases). 3. Read Write: threads ARM (tps) x86 (tps) ARM (tps - fscaled (FS)) % ARM-vs-x86 % ARM (FS)-vs-x86 1 137 149 158 -8 6 2 251 273 290 -8 6 4 462 502 533 -8 6 8 852 920 983 -7 7 16 1539 1678 1776 -8 6 32 2556 2906 2949 -12 1 64 3770 5158 4350 -27 -16 128 5015 8131 5787 -38 -29 256 5676 8562 6549 -34 -24 Analysis: Pattern is different with read-write workload. ARM starts lagging. Frequency scaling helps ease this lag for lower scalability but increasing scalability continues to increase the gap. 4. Update Index: threads ARM (tps) x86 (tps) ARM (tps - fscaled (FS)) % ARM-vs-x86 % ARM (FS)-vs-x86 1 328 373 378 -12 1 2 623 768 719 -19 -6 4 1060 1148 1223 -8 7 8 1905 2028 2198 -6 8 16 3284 3590 3789 -9 6 32 5543 6275 6396 -12 2 64 9138 10381 10544 -12 2 128 13879 16868 16014 -18 -5 256 19954 25459 23024 -22 -10 Analysis: Frequency scaled ARM continues to perform on par/better with x86 (except for heavy contention use-cases). 5. Update Non-Index: threads ARM (tps) x86 (tps) ARM (tps - fscaled (FS)) % ARM-vs-x86 % ARM (FS)-vs-x86 1 328 373 378 -12 1 2 588 686 678 -14 -1 4 1075 1118 1240 -4 11 8 1941 2043 2240 -5 10 16 3367 3662 3885 -8 6 32 5681 6438 6555 -12 2 64 9328 10631 10763 -12 1 128 14158 17245 16336 -18 -5 256 20377 26367 23512 -23 -11 Analysis: Frequency scaled ARM continues to perform on par/better with x86 (except for heavy contention use-cases). ConclusionThere are some important observations we can make: For read only workload MySQL on ARM continues to perform on-par with MySQL on x86. For write involving workload MySQL on ARM starts lagging a bit but if we consider frequency scaling things start getting better. Frequency scaling is not a real life parameter so we should consider the price-per-performance ratio. This could be a topic in itself but just a quick fact: ARM instance is 66% cheaper than x86 (24U48G same one we used). There is a pattern that we can observe. ARM workloads are very well scalable till it hits the CPU limits. With increasing scalability, contention increases and ARM starts lagging. This is expected since mutexes/contention hot-spots were all tuned for x86 (for example: spin-lock). But now that MySQL officially supports ARM and the growing ARM community and interest, it would be tuned for ARM too. To summarize, MySQL on ARM is a worth exploring option from a cost and performance perspective. If you have more questions/queries do let me know. Will try to answer them.","link":"/2020/04/08/mysql-on-x86-vs-arm/"},{"title":"Running MySQL on ARM. Does it work?","text":"作者: Krunal Bauskar 原文链接: https://mysqlonarm.github.io/Running-MySQL-on-ARM/ I am sure most of you may have this question. In fact, I too had it before I started working on #mysqlonarm initiative. What does it take to run MySQL on ARM? Does it really work? What about dependencies? What kind of performance does it have? What about support? Is there enough community support? This could go on….. Let’s try to answer these questions in simple question answer format. Q: Is MySQL supported on ARM?A: Yes, MySQL is officially supported on ARM. There are packages available that you can download from mysql.com site. Q: Which OS are supported?A: Currently support is enabled for RHEL-7 &amp; 8/Oracle-Linux- 7 &amp; 8. I don’t see direct package support for other OS. Q: Can we build it from source code for other OS (like say Ubuntu)?A: Yes. It works. I have been using binaries built from source code (using mysql-8.0.19 tag current release tag) on Ubuntu-18.04 (Bionic Beaver). (Also, build it on CentOS if you want to go the source code way). This also means all needed dependencies are taken care off or are already available. Q: Are supporting tools available on ARM?A: Since packages are available and I was able to build it from source too the default utilities like mysql shell/mysqladmin/mysqlslap/mysqldump/etc… and tons of other things that default ships along with binaries are available. If you care about a specific tools do let me know I will check them out. For now I have tried percona-toolkit some selective tools and they too work. Q: Does MariaDB and Percona too support their respective server flavor on ARM?A: MariaDB Community Server packages (from MariaDB corporation) are available for ARM (CentOS7/Ubuntu-16.04/18.04). Tools for MariaDB server are not yet officially available on ARM.Percona doesn’t yet officially support ARM but I was able to build it from source (MyRocks/TokuDB are not available). Q: Non-availability of tools. Can that block my progress of trying MySQL (or its variants) on ARM?A: No. Since most of these tools talk mysql protocol you can of-course install them on x86 with server running on ARM. (if tool is not yet ported to ARM) Q: Is there enough community support?A: MySQL on ARM is there for quite some time. There are active contributions from multiple vendors including ARM, Qualcomm, Huawei etc… and the community is growing rapidly. There is a lot of interest from all sections on optimizing MySQL on ARM. Lot of developers wanted to connect with this initiative. There are few challenges, most importantly non-availability of the hardware. If you are interested in contributing please talk to me (shoot me an email). Q: All that looks good. What about performance?A: This is a wide topic so I will be publishing multiple posts on this topic in the coming days but to put it in short performance is comparable. On other hand ARM instances should provide better price performance. Q: What about Support?Since packages are available officially from MySQL I presume their service offering should also cover ARM. Same with MariaDB. And of-course beyond official support there are common groups and independent developers. Command to build MySQL on ARM12cmake .. -DWITH_NUMA=1 -DDOWNLOAD_BOOST=1 -DWITH_BOOST=&lt;boost-dir&gt; -DCMAKE_INSTALL_PREFIX=&lt;dir-to-install&gt;make -j &lt;num-of-cores&gt; So no special flag is needed to build MySQL on ARM. (Assumes you have installed standard dependencies). It defaults compiles with “CMAKE_BUILD_TYPE=RelWithDebInfo” ConclusionMySQL on ARM is reality and it is now officially supported with ever growing eco-system/community. So give it a try. It could be your next cost-saving options without comprising performance or functionality. If you have more questions/queries do let me know. Will try to answer them","link":"/2020/03/31/running-mysql-on-arm-does-it-work/"},{"title":"Understanding InnoDB rw-lock stats","text":"作者: Krunal Bauskar InnoDB uses mutexes for exclusive access and rw-locks for the shared access of the resources. rw-locks are used to control access to the common shared resources like buffer pool pages, tablespaces, adaptive search systems, data-dictionary, informaton_schema, etc… In short, rw-locks play a very important role in the InnoDB system and so tracking and monitoring them is important too. InnoDB provides an easy way to track them using “SHOW ENGINE INNODB STATUS”. 1234RW-shared spins 38667, rounds 54868, OS waits 16539RW-excl spins 6353, rounds 126218, OS waits 3936RW-sx spins 1896, rounds 43888, OS waits 966Spin rounds per wait: 1.42 RW-shared, 19.87 RW-excl, 23.15 RW-sx In this article we will try to understand how these stats are calculated and what is the significance of each of these numbers. We will also try to draw inferences using different use-cases and touch base important stats bug that makes the current state of the stats almost ineffective for tuning. rw-lock spin algorithmThere are 3 types of rw-locks: Shared: offers shared access to the resource. Multiple shared locks are allowed. Exclusive: offers exclusive access to the resource. Shared locks wait for exclusive locks. Shared-Exclusive (SX): offer write access to the resource with inconsistent read. (relaxed exclusive). We will first try to understand the flow and then discuss some tuning steps. (For sake of discussion, to start with, let’s assume spins=0, rounds=0, os-waits=0). Locking Steps Step-1: Try to obtain the needed lock If SUCCESS then return immediately. (spins=0, rounds=0, os-waits=0) If FAILURE then enter spin-loop. Step-2: Start Spin-loop. Increment spin-count. (Why is spin-loop needed? If we enter wait then the OS will take away CPU from the given thread and then the thread will have to wait for its turns as per OS-scheduling. Better approach is to busy-wait using spin-loop (with condition check) so that the CPU is kept. Since most of these locks are short-duration likely chance of re-obtaining it very high). Step-3: Start spinning for N rounds. (Here N is defined and controlled by innodb_sync_spin_loops). Default is 30 rounds. Step-3a: Each round will invoke a PAUSE logic (see a separate section below about PAUSE logic) that will cause the CPU to go to PAUSE for X cycles. Step-3b: Post each round a soft check is done if the said lock is available (busy-wait). If it is available then spin-cycle exits. (There could still be some rounds pending. We will use this info below). Step-3c: Again try to obtain the needed lock. If SUCCESS then return. (spins=1, rounds=M (M &lt;= N), os-waits=0) If FAILURE and there are some pending rounds (max=innodb_sync_spin_loops) then resume spinning. (How come the loop was interrupted and locking failed. Note: the said lock is being looked upon by multiple threads in parallel. While multiple threads got signals about lock availability by the time said thread tried to obtain the lock, some other thread took it. So the said thread is now back re-trying). Step-3d: Say a thread now completes its set rounds of spin-wait and even now it failed to obtain the lock. There is no point in spinning further and wasting CPU cycles. Better give up pending CPU cycles back to OS and let OS scheduling do the needful. Also, since the said thread is now going to go to sleep it should register itself with some common infrastructure that will help signal it back to active whenever the said lock is available. Step-3e: This infrastructure to signal it back to active is sync-array infrastructure in InnoDB. Said thread registers itself by reserving a slot in the said array. Before starting the wait, give another try to see if the lock is available. (since reserving could be time consuming and lock could be available in meantime). If still lock is not available then wait for sync-array infrastructure to signal back the said thread. This wait is called OS-wait and entering this loop will now cause OS-waits count to increase. Step-3f: Say the said thread is signaled by sync-array infrastructure for the wait-event. It retries to obtain the needed lock. If SUCCESS then return. (spins=1, rounds=N, os-waits=1) If FAILURE then the whole loop restarts from spinning logic (Back to Step-3 with rounds-count re-intialize to 0). Note: spins count is not re-incremented. So let’s now assign the meaning to these counts spins: represent how many times flow failed to get a lock in first go and had to enter spin-loop. rounds: represent how many rounds of PAUSE logic executed. os-waits: how many times spin-loop failed to grant a lock that resulted in os-waits. It is possible that during a given spin loop for acquiring said lock flow may need more than 30 (innodb_sync_spin_loops) rounds of PAUSE logic and have to multiple time enter os-waits. This can cause os-waits &gt; spins-count. PAUSE logicK = {random value from between (0 - innodb_spin_wait_delay) * innodb_spin_wait_pause_multiplier} Invoke low-level PAUSE instruction K times. Not all architectures provide low-level pause instruction. x86 does provide it but ARM doesn’t. Even with x86 latency of this pause instruction continues to change with different families of processors. It was around 10-15 cycles for old generation processors (pre-skylake). Went upto 140 cycles with Skylake and again came down with CascadeLake (I see 13 cycles with Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz which belongs to CascadeLake family). (I have not personally benchmarked it on all platforms (except Cascadelake) but this is based on the references. This means the delay (in terms of cycle) introduced by PAUSE continues to change and so tuning PAUSE logic for each generation/type of processor is important. 2 configurable variables viz. innodb_spin_wait_delay, innodb_spin_wait_pause_multiplier exactly help achieve this. Interpreting statsNow that we understand the stats let’s look at the number and try to draw some inferences. But before we get into further details let me point out a bug that makes these stats inconsistent and incorrect. To get a fair idea we will use the version of mysql with the patch applied (as pointed in bug, stats w/o patch doesn’t help yield correct picture and so all kinds of interpretation and tuning is bound to fail). Use Case 11234RW-shared spins 338969, rounds 20447615, OS waits 592941RW-excl spins 50582, rounds 1502625, OS waits 56124RW-sx spins 12583, rounds 360973, OS waits 10484Spin rounds per wait: 60.32 RW-shared, 29.71 RW-excl, 28.69 RW-sx Let’s analyze the shared spins case: spins 338969, rounds 20447615, OS waits 592941```12345678910111213* 338K times flow couldn’t find the lock in first go, forcing thread to enter spin-lock.* During each spin-cycle there were 60 rounds of PAUSE cycle executed (so the said spin-cycles were done 2 times).* OS-waits/spins = 592/338 = 1.75 suggest that majority of the flow entered OS-wait (delay from PAUSE was not sufficient).* It also suggests that for the majority of spin-cycles, single OS -wait was not enough so it was repeated.**Conclusion:** Use-case represents heavy contention. Also, it sounds like PAUSE loop is unable to introduce the needed delay that is causing so many ROUNDS of PAUSE loop per spin-cycle.&lt;em&gt;256 thread oltp-read-write workload on 24 vCPU ARM machine&lt;/em&gt;### &lt;span style=&quot;color:#1aa260&quot;&gt;Use Case 2&lt;/span&gt; RW-shared spins 35943, rounds 777178, OS waits 19051RW-excl spins 4269, rounds 121796, OS waits 4164RW-sx spins 13407, rounds 321954, OS waits 7346Spin rounds per wait: 21.62 RW-shared, 28.53 RW-excl, 24.01 RW-sx 123456789101112131415161718Let’s analyze the shared spins case:&lt;br&gt;`RW-shared spins 35943, rounds 777178, OS waits 19051`* Flow invokes spin-loop 35K times.* Only 19K (that is approximately half of the spin-loop) caused OS-waits.* Per spin-cycle average too is limited to 21.62 that suggests that for each spin-cycle on average 22 rounds of PAUSE loop was invoked.**Conclusion:** Use-case represents medium contention.&lt;em&gt;16 thread oltp-read-write workload on 24 vCPU ARM machine&lt;/em&gt;### &lt;span style=&quot;color:#1aa260&quot;&gt;Use Case 3&lt;/span&gt;Just for reference, let me put an example of very less contention. This is with 16 threads oltp-read-write workload on x86_64 based VM with 16 CPU. RW-shared spins 39578, rounds 424553, OS waits 7329RW-excl spins 5828, rounds 78225, OS waits 1341RW-sx spins 11666, rounds 67297, OS waits 449Spin rounds per wait: 10.73 RW-shared, 13.42 RW-excl, 5.77 RW-sx 123456789101112* Flow invokes spin-loop 39K times.* Only 7K (that is approximately 20% of the spin-loop) caused OS-waits.* Per spin-cycle average too is limited to 10.**Conclusion:** Use-case represents low contention.&lt;em&gt;16 thread oltp-read-write workload on 24 vCPU x86_64 machine&lt;/em&gt;### &lt;span style=&quot;color:#1aa260&quot;&gt;Note on tuning&lt;/span&gt;Remember the high contention case we saw above. By tuning some things + code changes I could reduce the contention for shared-spins significantly. RW-shared spins 318800, rounds 13856732, OS waits 374634RW-excl spins 35759, rounds 656955, OS waits 22310RW-sx spins 10750, rounds 226315, OS waits 5598Spin rounds per wait: 43.47 RW-shared, 18.37 RW-excl, 21.05 RW-sx ``` Rounds per spins-cycles: average of 60 -&gt; 43 OS-wait per spin-cycle: 1.75 -&gt; 1.17 Is this good.? Not really. There are multiple factors to consider. Do you see improvement in TPS? Sometime it may be suggested to simply increase the PAUSE loop. But increasing PAUSE loop beyond some point can cause extended spin-cycle wasting precious CPU cycles especially if that causes it to land back in OS-wait. (Does this help more in HT cases vs multi-core case). Also, as pointed above, processor generation and type affect the PAUSE loop latency. There are multiple factors to consider. Even I am exploring this to see how we can tune this for all kinds of CPUs. I will blog more about it once I get some good solid algorithms on this front (or maybe we can develop some automated, self-adjustable or adaptive algorithms) so users don’t need to worry about it. ConclusionAs we saw above rw-locks stats can help us get good insight on understanding the contention of the system. Of-course it is not the only in-sight about InnoDB contention as mutexes are not covered as part of these stats. Tuning could be challenging but over-tuning can affect performance in the wrong way too. If you have more questions/queries do let me know. Will try to answer them.","link":"/2020/04/14/understanding-innodb-rw-lock-stats/"},{"title":"Web开源服务之ARM64现状","text":"作者: 王玺源 社区核心参与者：Martin Grigorov、Michael Rumph 背景开源界中Web服务众多,但其中很多软件对ARM64的支持并不理想。或是没有官方CI测试保证代码质量，或是在ARM64上的性能明显差于X86_64，甚至有的服务根本无法在ARM上运行。为了完善Web领域的ARM64生态，我们参与了主流的几个开源社区，旨在推动Web on ARM64。以下是我们近期的一些进展，以供大家参考。 概述我们目前参与了主流的共9个Web相关项目。如下所示 项目 主要语言 Apache Httpd Server C Apache Tomcat Java Memcached C Nginx C Lighttpd C JBoss/WildFly Java HAProxy C Squid C++ Varnish Cache C 针对这些项目，我们按照以下三个方面循序渐进的推动中: 能不能在ARM上运行 如何稳定在ARM上运行 怎么更好的在ARM上运行 能不能在ARM上运行我们可以看到这9大项目主要由Java和C/C++编写。 首先，像Python、Java这种自带runtime的语言天生就是跨平台的。这样的项目在ARM64平台上至少可以保证程序的可运行。 而C/C++项目则需要先编译成ARM64平台的目标可执行文件。这样的项目则需要先进行编译测试。 经过我们的测试，这9个Web项目都可以在ARM64上成功编译并运行。 如何稳定在ARM上运行所谓稳定，包含两个方面： 软件在ARM64上是否和在X86_64上行为一致？ 随着代码更新迭代，软件在ARM64上是否持续可用？ 行为一致我们常遇到两类行为一致的问题： 同样的代码，不同的结果 同样的功能，不同的支持 很遗憾，由于架构不同、底层实现不同等原因，很多软件的某些行为在X86_64和ARM64上的行为并不一致。 例如，之前的文章提到的Java中Math计算结果的差异。 又或者某些功能依赖独有的平台特性或者特殊的第三方库，导致在X86_64上可以运行的功能，在ARM64上却执行失败。 例如我们发现WildFly官方发布的源码包中缺少了个别ARM64平台的.so文件，这就导致个别调用.so的功能不可用。 针对这种问题，我们需要打开代码逐个分析、逐个修复。保证所有测试在ARM64上全部通过。 持续可用CI/CD是保证软件持续可用的重要方法。主流软件的CI系统都有X86_64平台的测试。而ARM64平台的少之又少。 针对这个问题，我们推动了这9个项目的ARM CI支持。除Lighttpd还在推动中以外，其他8个项目目前都已支持了ARM CI。甚至其中4个项目已经官方声明了ARM64的支持（详见附录）。 其中Httpd、Tomcat、Memcached、HAProxy和Varnish Cache通过Travis CI支持了ARM64测试。Nginx使用内部CI，对外不可见。Squid使用自己的树莓派。而JBOSS使用了我们捐献的基于Kunpeng 920的ARM虚拟机。同时我们也计划捐献同样的测试机到Lighttpd社区中。 随着ARM CI的落地，我们将持续保证ARM CI的稳定。我们相信在不久的将来，这9大核心Web项目都会官方声明ARM64的支持，并满足用户在ARM64上稳定、高效使用Web服务的需求。 怎么更好的在ARM上运行我们不仅希望软件在ARM64上能用，还在不断探索如何让软件在ARM64上用的好。其中性能优化是重中之重，也是我们未来一段时间的主要投入点。 例如，有些软件只实现了X86_64的汇编实现，但缺少ARM64的汇编代码。 又或者有些在X86_64上纯软实现的功能，可以在ARM64上通过下沉至硬编码的方式提高性能。 甚至还可以考虑如何最大化利用ARM64的多核优势，或规避ARM64的锁劣势等等。 关于性能优化的内容，我们将在以后的文章中针对不同的软件一一细说。敬请期待。 附录最后附上我们参与Web社区的总览表格及相关链接，感兴趣的同学可以进一步详读，有任何问题，欢迎留言。 Official arm64 CI CI tool Package in Downloads Official ARM support Apache Tomcat YES TravisCI Binary WIP(work in progress) Memcached YES 1. BuildBot 2. TravisCI Source Code YES Apache httpd YES TravisCI Source Code YES NGINX YES Internal Only for Ubuntu LTSs YES Lighttpd NO Jenkins Source Code NO JBoss/Wildfly YES TeamCity Source Code NO HAProxy YES 1. CirrusCI 2. TravisCI Source Code YES Squid YES Jenkins Source Code NO Varnish Cache YES Travis 1. Source Code 2. Package NO","link":"/2020/04/10/web-kai-yuan-fu-wu-zhi-arm64-xian-zhuang/"},{"title":"Why ARM?","text":"作者: Krunal Bauskar 原文链接: https://mysqlonarm.github.io/Why-ARM/ ARM processors are everywhere. It is quite likely some of you may be reading this blog from an ARM powered device. Phone, IoT devices, consumer and home appliances, health-care devices, all are powered by ARM processors. ARM processors are known to be power efficient and so most of these devices that demands a long recharge cycle but less processing power started using them. But this has changed in the past few years. More and more ARM processors are being used for high-end applications like database server, web server, application server, big data use-cases. They have already made their way to the data-centers as a server class machines. They are being looked upon as a cost effective option while running applications in cloud. ARM ecosystem evolutionFew years back it was difficult to imagine that ARM would be used for running some high-end server class applications. There were 2 major reasons that I could think off: ARM were best suited for small handheld devices. ARM ecosystem was limited around the specific product it supported. ARM ecosystem has really picked up well after some major OS providers added support for it including RedHat (CentOS), Ubuntu, Debian, Windows. This eased out porting of the major softwares to ARM. ARM community gave it a push to make sure most of the standard softwares are available on ARM viz. IDE, DB-server, Hadoop and all its variants from Apache Foundation, CI/CD software, Container, Virtualization, etc… The ARM model that allows other vendors to license and develop their own ARM processors further helped fueled its popularity with more chip designers joining, collaborating and innovating. Break-through came with major cloud providers like Amazon started providing ec2 instances (currently invitation only) based on ARM processors this means now everyone can boot an ARM instance and start developing/porting their software on ARM. This helped further grow the ecosystem. What was missing?Though most of these software have been ported to ARM they were not yet optimized for ARM. ARM has a weak memory model, can fit more cores in smaller space, difference in low-level instruction (for software that uses them), etc.. This was the start of the 2nd phase of ARM where the community/developer/user started moving from “running software on arm” -&gt; “optimizing software on arm”. I think this was a major win for the arm community when users started to think ARM seriously and started spending efforts on optimizing their software on ARM. This (especially optimization) is a never ending process but I see first goal is to at-least be on par with x86. I purposely say “onpar” because each of architecture has its own USP so say if you port an enterprise class application to ARM and you can offer it to customer @ 50% of the cost (operating cost + initial investment) for 75% of the performance of x86 I think that would be still be attractive fit for most of the customers (especially given application are horizontally scalable). Of-course that doesn’t mean all applications run on ARM at reduced speed, in fact there are applications that run on ARM faster than x86 and since the optimization phase has just started in next few years a lot of applications would be running on ARM faster than other architectures. Go GreenIt is everywhere and especially a matter of major concern for data-center operators (small or big). ARM being power efficient can save approximately 50% of the power compared to other architecture. This makes it help support Go-Green initiative. ARM is Next-Gen processorIt is interesting why I referred to it this way. Next generation kids are actively using kits like Andrino, Raspberry Pi, Odroid, Banana Pi, Asus tinker board, etc…. to build some of the next-gen system. These kids will be defining the next generation of computing. Given they started with ARM their social community has grown around ARM in the next few years there would be an army of ARM users/developers with a very active community. All the groundwork and good things that are being built at this stage around ARM will be pushed to the next level once this workforce becomes active. ARM in Desktop/LaptopThis is catching up fast and no wonder if we start seeing ARM based Desktop/PC workstation/Laptop (there are already few) commonly being used. ConclusionThe ARM Ecosystem looks a lot more fascinating and full of new challenges and opportunities. Current decade will be ruled by ARM based processors and it will be everywhere from tiny wearable devices to high-end movie experience, from auto-driving cycle/car to jumbo jet/space-craft. It is estimated that there would be 35 active ARM power devices per person. That’s Ocean of Opportunity. If you have any comments feel free to drop an email (check about section)","link":"/2020/03/30/why-arm/"},{"title":"理解InnoDB rw-lock的统计数据","text":"译者：bzhaoopenstack 原文链接：https://mysqlonarm.github.io/Understanding-InnoDB-rwlock-stats/ 作者: Krunal Bauskar InnoDB使用互斥锁进行独占访问，使用rw-locks进行共享访问。rw-locks用于控制对缓冲池页、表空间、自适应搜索系统、数据字典、informaton_schema等公共共享资源的访问。总之，rw-locks在InnoDB系统中扮演着非常重要的角色，因此跟踪和监视它们也很重要。 InnoDB 提供了一种简单的方式来跟踪它们， “SHOW ENGINE INNODB STATUS”. 1234RW-shared spins 38667, rounds 54868, OS waits 16539RW-excl spins 6353, rounds 126218, OS waits 3936RW-sx spins 1896, rounds 43888, OS waits 966Spin rounds per wait: 1.42 RW-shared, 19.87 RW-excl, 23.15 RW-sx 在本文中，我们将了解这些统计数据是如何计算的，以及每个数据的意义。我们还将尝试使用不同的用例来描述一些推论，并且接触一下基础又重要的统计数据， 这个bug使当前状态的统计几乎无法进行调优。 rw-lock 自旋算法rw-locks有三种类型: Shared: 提供资源的共享访问。允许多个共享锁。 Exclusive: 提供对资源的独占访问。共享锁等待排他锁。 Shared-Exclusive (SX): 对读不一致的资源提供写访问（relaxed exclusive）。 首先我们先尝试理解流程，然后讨论一些调优步骤。 (为了便于讨论，假设 spins=0, rounds=0, os-waits=0). Locking 步骤 Step-1: 尝试获取所需的锁 If SUCCESS then return immediately. (spins=0, rounds=0, os-waits=0) If FAILURE then enter spin-loop. Step-2: 开始自旋回环(Spin-loop). 增加自旋计数 (spin-count). (为什么需要自旋循环？如果我们的线程进入等待状态，那么操作系统将把CPU从给定的线程中带走，暂时不让它使用CPU，然后线程将不得不按照操作系统调度的次序等待CPU资源，从而进行下面的任务。更好的方法是在繁忙等待中（busy-wait）使用自旋循环(带有条件的检查确认锁是否被释放）以便保留CPU。由于这些锁大多数都是短时间使用，因此重新获得的机会可能性非常高。). Step-3: 开始N轮自旋。Start spinning for N rounds. (这里N的定义由innodb_sync_spin_loops来控制)。默认为30轮。 Step-3a: 每一轮将调用一个PAUSE逻辑（见下面关于PAUSE逻辑的单独一节），这将导致CPU进入PAUSE的X个周期。 Step-3b: 每轮检查（软实现）是否对应的锁已经可用(busy-wait)。 如果它可用，那么自旋循环退出。（可能还有一些其他同样正在自旋的检查。我们将使用下面的信息）。 Step-3c: 再次尝试获取所需的锁。 If SUCCESS then return. (spins=1, rounds=M (M &lt;= N), os-waits=0) If FAILURE，并且此时仍有其他正在自旋，且悬而未决任务 (max=innodb_sync_spin_loops)还是继续自旋。（为什么循环被中断，锁失败。注：该锁被多个线程并行查看。而当多个线程试图获取锁时，它们收到了锁可用的信号。被其他线程取走，所以该线程现在仍在重新尝试）。 Step-3d: 当这个线程现在完成了它设置的spin-wait轮循次数，到现在它还没有获得锁。那么它会被认为浪费CPU周期，没有必要继续自旋。最好的选择是放弃挂起的CPU周期并交还给操作系统，让操作系统调度做其他有用的事情。此外，由于所述线程现在将要进入睡眠，它应该向一些公共基础设施注册自身，这些基础设施将帮助它在所述锁可用时发出恢复活动的信号。 Step-3e: 这个将线程从唤醒的基础设施正式InnoDB中的同步阵列（sync-array）基础设施。 所述线程通过在同步阵列中预留插槽来注册自身。 在开始等待之前，再试一次看锁是否可用。（因为预留可能很费时间，同时锁这个时候是可用的状态）。 如果仍然没有获得锁，则将等待同步阵列向该线程送回信号。 这种等待称为OS-wait，进入这个循环现在会导致OS-waits计数增加。 Step-3f: 如果该线程收到由同步阵列发送回的wait-event信号。它会重新尝试获取锁。 If SUCCESS then return. (spins=1, rounds=N, os-waits=1) If FAILURE ，则整个循环从旋转逻辑重新启动(返回Step-3，rounds-count重新初始化为0)。注意：自旋计数(spins count)不会重新递增。 所以现在我们来给这些计数赋予意义 spins: 代表在第一次尝试中多少轮数而未能得到一个锁，并不得不进入自旋循环。 rounds: 表示执行多少轮PAUSE逻辑。 os-waits:自旋循环在多少轮自旋时仍未得到锁而导致os-waits。 在获取所述锁流程中的自旋循环期间中，可能需要超过30轮(innodb_sync_spin_loops)PAUSE逻辑，并且还可能多次进入os-waits。这可能会导致os-waits &gt; spins-count。 PAUSE 逻辑K = {取 (0 - innodb_spin_wait_delay)之间的随机数 * innodb_spin_wait_pause_multiplier} 调用底层 PAUSE 指令 K 次. 并不是所有的架构都提供底层的PAUSE指令。x86有提供，但ARM没有。即使x86深藏了这个PAUSE指令，它也会随着处理器的不同系列而继续变化。老一代处理器的周期约为10-15次（pre-skylake）,Skylake系列的周期约为140次，然后CascadeLake系列的周期数又降下来了 (我看到在属于CascadeLake系列的Intel(R) Xeon(R) Gold 6266C CPU @ 3.00GHz芯片上是13次). (除了Cascadelake系列以外，我个人并没有在其他平台上对它进行基准测试) 但这些信息是可供参考的。这意味着延迟引入PAUSE指令（按照周期计算）会持续不断的变化，所以针对每一代/类型的处理器调整PAUSE逻辑是非常重要的。 这里有两个可配置的变量可以解决这个问题，innodb_spin_wait_delay 和 innodb_spin_wait_pause_multiplier。 统计数据解读现在我们已经了解了统计数据，让我们看看这个数字，并试着做出一些推断。 不过，在谈及进一步的细节之前，让我先看看这个bug， 它描述了导致这些统计数据不一致和不正确的原因和修复方法。 为了得到一个公正的结论，我们将使用mysql对应版本，并打上补丁。 (正如bug中指出的，不使用修复补丁统计数据不能产生正确的数据，因此各种解释和调优都毫无用处). Use Case 11234RW-shared spins 338969, rounds 20447615, OS waits 592941RW-excl spins 50582, rounds 1502625, OS waits 56124RW-sx spins 12583, rounds 360973, OS waits 10484Spin rounds per wait: 60.32 RW-shared, 29.71 RW-excl, 28.69 RW-sx 让我们分析一下共享自旋的情况: 1RW-shared spins 338969, rounds 20447615, OS waits 592941 在头一次尝试中，用了338K 次仍未获取到锁，迫使线程进去自旋锁状态(spin-lock)。 在每个自旋周期内，执行了60轮PAUSE周期（因此，所述自旋周期执行了2次）。 OS-waits/spins = 592/338 = 1.75表明大部分被分流进入了OS-wait（PAUSE的延迟不够）。 表明对于大多数自旋周期，单一的操OS-wait是不够的，因此这种操作是在重复进行的。 Conclusion: 该Use-case是高竞争情况。而且，诸如PAUSE循环无法产生所需的延迟来获得锁，导致每个自旋周期产生如此之多的PAUSE循环。 256 thread oltp-read-write workload on 24 vCPU ARM machine Use Case 21234RW-shared spins 35943, rounds 777178, OS waits 19051RW-excl spins 4269, rounds 121796, OS waits 4164RW-sx spins 13407, rounds 321954, OS waits 7346Spin rounds per wait: 21.62 RW-shared, 28.53 RW-excl, 24.01 RW-sx 让我们分析一下共享自旋的情况: 1RW-shared spins 35943, rounds 777178, OS waits 19051 流程中，自旋循环35K次。 只有19K次(大约是自旋循环的一半)引起了OS-waits。 平均每个自旋周期也限制在21.62，这表明，对于每个自旋周期，平均有22轮PAUSE循环。 Conclusion: 该Use-case表示中度竞争情况。 16 thread oltp-read-write workload on 24 vCPU ARM machine Use Case 3让我举一个常见的例子，以供参考。这是16个线程的oltp-read-write工作负载在基于x86_64的16CPU虚拟机上。 1234RW-shared spins 39578, rounds 424553, OS waits 7329RW-excl spins 5828, rounds 78225, OS waits 1341RW-sx spins 11666, rounds 67297, OS waits 449Spin rounds per wait: 10.73 RW-shared, 13.42 RW-excl, 5.77 RW-sx 流程中自旋循环39K次。 只有7K(约占自旋循环的20%) 导致OS-waits。 每自旋周期平均数也限制为10。 Conclusion: 该Use-case表示低竞争情况。 16 thread oltp-read-write workload on 24 vCPU x86_64 machine 调优注意事项记得我们在上面看到的高竞争案例。通过优化一些代码，可以显著减少共享自旋的争用。 1234RW-shared spins 318800, rounds 13856732, OS waits 374634RW-excl spins 35759, rounds 656955, OS waits 22310RW-sx spins 10750, rounds 226315, OS waits 5598Spin rounds per wait: 43.47 RW-shared, 18.37 RW-excl, 21.05 RW-sx 每个自旋周期的轮数: 平均数从 60 降到 43 每个自旋周期的OS-wait次数: 从1.75 降到1.17 这性能好起来了吗？不是太好。有许多因素需要考虑。 你看到TPS有改善吗？ 有时，它可能会建议简单地增加PAUSE循环。但是，增加PAUSE循环超过某个点将会导致延长自旋周期，最终浪费宝贵的CPU周期，尤其是这会导致线程返回到OS-wait状态。(这种方式可能对HT案例和多核案例更有效)。 同样，如上所述，不同处理器的系列和类型会影响PAUSE循环延迟。 有许多因素需要考虑。甚至我正在研究这个问题，看看我们如何为所有类型的CPU来优化它。一旦我在这个研究中发掘到一些非常好的通用的算法(或者我们可以开发一些自动的、自调整的或自适应的算法)，我会发布更多关于这个问题的博客，用户无需担心。 结论正如我们在上文看到的，rw-locks统计数据可以帮助我们更好地理解系统中锁的争用。当然，它不是有关InnoDB争用的唯一说明，因为互斥锁没在这些统计数据里面。调优可能具有挑战性，因为以错误的方式过度调优也会影响性能。 如果你还有问题/疑问，请告诉我。会试着去回答他们。","link":"/2020/04/15/li-jie-innodb-rw-lock-de-tong-ji-shu-ju/"},{"title":"让Github Action在你的ARM机器上跑起来","text":"作者：姜逸坤 Github在2019年8月，宣布推出了一项新的功能——Github Action，让成千上万的开源项目可以利用Github提供的计算资源完成构建、测试、部署等CI/CD，并且提供Self Hosted Runners功能，让开发者们可以将自己的机器接入到Github中来。 最近，我们利用这一功能，将搭载着openEuler 20.03 (LTS) 操作系统，跑在Kunpeng 920 处理器的ARM环境接入进来，在近期华为与阿里合作的MPAM项目，也将充分的利用这些资源利用Github Action的能力完成构建与测试。 本篇文章将接入方法分享给大家，希望能够帮助更多同学们把自己的ARM环境也在Github上用起来。 1. 接入资源资源的接入流程比较简单： 依次点击项目的Settings–Actions进入资源接入页面，点击Add Runner。 根据弹出的提示，下载和运行脚本 完成后我们可以看到接入的资源： 2. 使用资源我们为接入的项目增加一个Action： 123456789101112131415161718192021name: Run some script in Kunpeng envon: push: branches: [ master ] pull_request: branches: [ master ]jobs: build: runs-on: self-hosted steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v2 - name: Run `uname -a` in Kunpeng env run: | uname -a cat /etc/os-release lscpu | grep -E \"Architecture|Model name|CPU\\(s\\):\" 这样，这个workflow是展示所接入的环境上内核、操作系统、处理器信息，我们可以从结果看到job的结果： 点击Details可以进入详情页面： 可以看到，我们在资源上执行的指令，已经运行成功，可以看到这台资源的系统为openEuler 20.03 (LTS)，CPU为aarch64 128核的Kunpeng 920。 3.结语本文介绍了我们是如何将搭载着鲲鹏920处理器、openEuler操作系统的计算资源接入到Github Action的。可以看到Github Action的自定义资源接入，在ARM64下还是很顺滑的。 希望这篇文章能够帮助到大家，大家也可以尝试着将你们自己ARM资源接入进来，有问题可以留言一起讨论，玩的开心！：）","link":"/2020/04/15/rang-github-action-zai-ni-de-arm-ji-qi-shang-pao-qi-lai/"},{"title":"让大数据生态在ARM架构下更顺滑","text":"作者：郑振宇 受疫情影响Linaro Connect 2020改为线上直播的Linaro Tech Days，笔者所在团队在该活动上介绍了自19年Q4以来笔者团队在各主流开源社区推广ARM生态所做的工作以及所取得的成果。直播活动约有120+与会者。 视频回放：视频连接PPT：LTD20-104 Make life easier for Big Data users on ARM - Our efforts and future plans","link":"/2020/03/30/rang-da-shu-ju-sheng-tai-zai-arm-jia-gou-xia-geng-shun-hua/"},{"title":"鲲鹏计算团队博客开张啦，欢迎投稿！","text":"我们将在这里分享关于鲲鹏计算相关的技术、开源、生态的点滴。 欢迎关注！欢迎转发！欢迎投稿！ 如何投稿？非常简单，仅需要两步： 点击这里，进入博客提交页面，我们使用Issue对博客进行管理，点击New进行投稿。 填写标题和内容，issue标题即为文章标题，issue内容即为文章内容，并发布请求。 好了，至此你的投稿已经完成，你可以在这里看到你的投稿，并进行迭代修改。 如何发布？等到管理员审核通过后，会将你issue打上publish标签，之后，你内容就会自动同步在博客中啦！ 来吧，还等什么？把你的干货分享起来！","link":"/2020/03/27/kun-peng-ji-suan-tuan-dui-bo-ke-kai-zhang-la-huan-ying-tou-gao/"}],"tags":[{"name":"虚拟化","slug":"虚拟化","link":"/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"基础库","slug":"基础库","link":"/tags/%E5%9F%BA%E7%A1%80%E5%BA%93/"},{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Web","slug":"Web","link":"/tags/Web/"},{"name":"会议","slug":"会议","link":"/tags/%E4%BC%9A%E8%AE%AE/"},{"name":"大数据","slug":"大数据","link":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"categories":[{"name":"虚拟化","slug":"虚拟化","link":"/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"},{"name":"基础库","slug":"基础库","link":"/categories/%E5%9F%BA%E7%A1%80%E5%BA%93/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Web","slug":"Web","link":"/categories/Web/"},{"name":"大数据","slug":"大数据","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]}